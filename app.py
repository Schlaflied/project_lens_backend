# -*- coding: utf-8 -*-
# -----------------------------------------------------------------------------
# ã€ŒèŒåœºé€é•œã€åç«¯æ ¸å¿ƒåº”ç”¨ (Project Lens Backend Core)
# ç‰ˆæœ¬: 27.0 - å¼•ç”¨é“¾æ¥å¢å¼ºç‰ˆ (Clickable Citation Version)
# æè¿°: 1. (å·²å®ç°) å®Œæ•´çš„å¼•ç”¨é˜²å¹»è§‰ä¸å‡€åŒ–æœºåˆ¶ã€‚
#       2. (æœ¬æ¬¡æ›´æ–°) æ–°å¢äº†ä¸€ä¸ªå¤„ç†æ­¥éª¤ï¼Œåœ¨å‡€åŒ–æŠ¥å‘Šåï¼Œè‡ªåŠ¨å°†æ‰€æœ‰æœ‰æ•ˆçš„
#          å¼•ç”¨è§’æ ‡ [id] è½¬æ¢ä¸ºå¯ç‚¹å‡»çš„Markdowné“¾æ¥æ ¼å¼ [id](url)ï¼Œ
#          æ–¹ä¾¿å‰ç«¯ç›´æ¥æ¸²æŸ“ä¸ºè¶…é“¾æ¥ã€‚
# -----------------------------------------------------------------------------

import os
import requests
import google.generativeai as genai
import time
import re
import json
from flask import Flask, request, jsonify
from flask_cors import CORS
from bs4 import BeautifulSoup
from flask_limiter import Limiter
from flask_limiter.util import get_remote_address
import traceback
import datetime

# --- 1. åˆå§‹åŒ–å’Œé…ç½® ---
app = Flask(__name__)
CORS(app, resources={r"/analyze": {"origins": "*"}})
limiter = Limiter(get_remote_address, app=app, default_limits=["200 per day", "50 per hour"], storage_uri="memory://")

# --- 2. APIå¯†é’¥é…ç½® ---
try:
    GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
    SEARCH_API_KEY = os.getenv("SEARCH_API_KEY")
    SEARCH_ENGINE_ID = os.getenv("SEARCH_ENGINE_ID")
    if not all([GEMINI_API_KEY, SEARCH_API_KEY, SEARCH_ENGINE_ID]): raise ValueError("APIå¯†é’¥ç¼ºå¤±")
    genai.configure(api_key=GEMINI_API_KEY)
    print("APIå¯†é’¥é…ç½®æˆåŠŸï¼")
except Exception as e:
    print(f"APIå¯†é’¥é…ç½®å¤±è´¥: {e}")

# --- 3. æ™ºèƒ½æå–å®ä½“ (æ— å˜åŒ–) ---
def extract_entities_with_ai(text_blob):
    print("ğŸ¤– å¯åŠ¨AIå®ä½“æå–ç¨‹åº (å«åœ°ç‚¹)...")
    try:
        model = genai.GenerativeModel('gemini-1.5-pro')
        prompt = (f'From the text below, extract the company name, job title, and location. Respond with a JSON object: {{"company_name": "...", "job_title": "...", "location": "..."}}.\nIf a value isn\'t found, return an empty string "".\n\nText:\n---\n{text_blob}\n---\n')
        response = model.generate_content(prompt, generation_config=genai.GenerationConfig(response_mime_type="application/json"))
        if not response.parts: print(f"--- å®ä½“æå–AIå“åº”è¢«é˜»æ­¢: {response.prompt_feedback} ---"); return text_blob, "", ""
        entities = json.loads(response.text)
        company, job_title, location = entities.get("company_name", ""), entities.get("job_title", ""), entities.get("location", "")
        print(f"âœ… AIæå–æˆåŠŸ: å…¬å¸='{company}', èŒä½='{job_title}', åœ°ç‚¹='{location}'")
        return company if company else text_blob, job_title, location
    except Exception as e:
        print(f"âŒ AIå®ä½“æå–å¤±è´¥: {e}. å°†ä½¿ç”¨åŸå§‹æ–‡æœ¬ã€‚"); return text_blob, "", ""

# --- 4. Googleæœç´¢ (æ— å˜åŒ–) ---
def perform_google_search(query, api_key, cse_id, num_results=2):
    url = "https://www.googleapis.com/customsearch/v1"
    params = {'key': api_key, 'cx': cse_id, 'q': query, 'num': num_results, 'sort': 'date'}
    try:
        response = requests.get(url, params=params); response.raise_for_status()
        search_results = response.json()
        snippets = [item.get('snippet', '') for item in search_results.get('items', [])]
        sources = [{'title': item.get('title'), 'link': item.get('link')} for item in search_results.get('items', [])]
        return snippets, sources
    except requests.exceptions.RequestException as e:
        print(f"Googleæœç´¢è¯·æ±‚å¤±è´¥: {e}"); return [], []

# --- 5. ç½‘é¡µçˆ¬è™« (æ— å˜åŒ–) ---
def scrape_website_for_text(url):
    try:
        headers = { 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36' }
        response = requests.get(url, headers=headers, timeout=10); response.raise_for_status()
        soup = BeautifulSoup(response.content, 'html.parser')
        [s.decompose() for s in soup(['script', 'style'])]
        text = '\n'.join(chunk for chunk in (phrase.strip() for line in (line.strip() for line in soup.get_text().splitlines()) for phrase in line.split("  ")) if chunk)
        return text[:5000]
    except Exception as e:
        print(f"âŒ çˆ¬å–ç½‘ç«™æ—¶å‘ç”Ÿé”™è¯¯: {e}"); return None

# --- 6. æ ¸å¿ƒAIæŒ‡ä»¤ (Prompt) [æ— å˜åŒ–] ---
PROMPT_TEMPLATE = (
    "As 'Project Lens', an expert AI assistant, generate a detailed analysis report in {output_language} as a JSON object.\n"
    "**Citation Rules (VERY IMPORTANT):**\n"
    "1. Cite information by embedding the corresponding source tag (e.g., `[1]`, `[2]`) provided in the `Research Data`.\n"
    "2. **You MUST ONLY use the source IDs provided in the `Research Data` section. DO NOT invent, hallucinate, or create any source IDs that are not explicitly given to you.**\n"
    "3. Include all genuinely used IDs in the final `cited_ids` array.\n"
    "**Information Provided:**\n"
    "1. **Company, Role & Location:** {company_name} - {job_title} in {location}\n"
    "2. **Current Date:** {current_date}\n"
    "3. **Applicant's Resume/Bio:**\n   ```{resume_text}```\n"
    "4. **Research Data (Each block has a `[Source ID: X]`):**\n   ```{context_with_sources}```\n"
    "**Your Task:** Synthesize all info into a single JSON object with the following structure:\n"
    "```json\n"
    "{{\n"
    '  "report": {{\n'
    '    "company_location": "{location}",\n'
    '    "red_flag_status": "Your assessment (e.g., \'Low Risk\').",\n'
    '    "red_flag_text": "Detailed explanation for red flags. Cite sources.",\n'
    '    "hiring_experience_text": "Analysis of hiring process. Cite sources.",\n'
    '    "timeliness_analysis": "1. Analyze info recency. 2. Analyze job posting status (e.g., \'Likely open\', \'Potentially expired\') and give a reason. Cite sources.",\n'
    '    "culture_fit": {{ "reputation": "", "management": "", "sustainability": "", "wlb": "", "growth": "", "salary": "", "overtime": "", "innovation": "", "benefits": "", "diversity": "", "training": "" }},\n'
    '    "value_match_score": "A number from 0-100. 0 if no resume.",\n'
    '    "value_match_text": "Explanation of the match score. Cite sources.",\n'
    '    "final_risk_rating": "Your final risk rating.",\n'
    '    "final_risk_text": "Summary justifying the final rating. Cite sources."\n'
    '  }},\n'
    '  "cited_ids": []\n'
    "}}\n"
    "```"
)

# --- 7. å¼•ç”¨å‡€åŒ–ä¸å¢å¼ºè¾…åŠ©å‡½æ•° ---

def extract_all_mentioned_ids(report_data):
    all_text = json.dumps(report_data)
    found_ids = re.findall(r'\[([\d,\s]+)\]', all_text)
    id_set = set()
    for id_group in found_ids:
        ids = [int(i.strip()) for i in id_group.split(',') if i.strip().isdigit()]
        id_set.update(ids)
    return id_set

def scrub_invalid_citations(data, valid_ids_set):
    """
    ã€ç»ˆæå‡çº§ã€‘é€’å½’åœ°æ¸…ç†æŠ¥å‘Šä¸­çš„æ— æ•ˆå¼•ç”¨ã€‚
    è¿™ä¸ªç‰ˆæœ¬æ›´å¼ºå¤§ï¼Œå¯ä»¥æ™ºèƒ½å¤„ç†AIå¯èƒ½æ’å…¥çš„Markdownæ ¼å¼ï¼ˆå¦‚æ¢è¡Œç¬¦æˆ–åˆ—è¡¨æ ‡è®°ï¼‰ã€‚
    """
    if isinstance(data, dict):
        return {k: scrub_invalid_citations(v, valid_ids_set) for k, v in data.items()}
    elif isinstance(data, list):
        return [scrub_invalid_citations(elem, valid_ids_set) for elem in data]
    elif isinstance(data, str):
        def repl(match):
            context_window = data[:match.start()].rstrip()
            context_check = context_window[-30:]
            citation_id = int(match.group(1))
            if citation_id in valid_ids_set and re.search(r'\w', context_check):
                return match.group(0)
            print(f"ğŸ‘» å‡€åŒ–å¹½çµå¼•ç”¨: ç§»é™¤äº†æ— æ•ˆæˆ–å­¤ç«‹çš„å¼•ç”¨è§’æ ‡ [{citation_id}]")
            return ""
        return re.sub(r'\[(\d+)\]', repl, data)
    else:
        return data

# --- V27.0 æ–°å¢ä¿®æ”¹å¼€å§‹ ---
def enrich_citations_with_links(data, source_map):
    """
    ã€æ–°å¢åŠŸèƒ½ã€‘é€’å½’åœ°æŸ¥æ‰¾æœ‰æ•ˆçš„å¼•ç”¨è§’æ ‡ [id]ï¼Œå¹¶å°†å…¶è½¬æ¢ä¸ºå¯ç‚¹å‡»çš„Markdowné“¾æ¥æ ¼å¼ [id](url)ã€‚
    è¿™æ ·ï¼Œå‰ç«¯å°±å¯ä»¥è½»æ¾åœ°å°†å®ƒä»¬æ¸²æŸ“æˆè¶…é“¾æ¥äº†ã€‚
    """
    if isinstance(data, dict):
        return {k: enrich_citations_with_links(v, source_map) for k, v in data.items()}
    elif isinstance(data, list):
        return [enrich_citations_with_links(elem, source_map) for elem in data]
    elif isinstance(data, str):
        # å®šä¹‰ä¸€ä¸ªå†…éƒ¨æ›¿æ¢å‡½æ•°
        def repl(match):
            citation_id = int(match.group(1))
            # æ£€æŸ¥è¿™ä¸ªIDæ˜¯å¦å­˜åœ¨äºæˆ‘ä»¬çš„æºæ•°æ®åœ°å›¾ä¸­ï¼Œå¹¶ä¸”æœ‰å¯¹åº”çš„é“¾æ¥
            if citation_id in source_map and source_map[citation_id].get('link'):
                url = source_map[citation_id]['link']
                # æ›¿æ¢ä¸ºMarkdowné“¾æ¥æ ¼å¼
                return f'[{citation_id}]({url})'
            else:
                # å¦‚æœå› ä¸ºæŸäº›åŸå› ï¼ˆç†è®ºä¸Šåœ¨æ¸…ç†åä¸ä¼šå‘ç”Ÿï¼‰æ‰¾ä¸åˆ°é“¾æ¥ï¼Œå°±ä¿æŒåŸæ ·
                return match.group(0)
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æŸ¥æ‰¾æ‰€æœ‰çš„ `[æ•°å­—]` æ ¼å¼å¹¶åº”ç”¨ä¸Šé¢çš„æ›¿æ¢å‡½æ•°
        return re.sub(r'\[(\d+)\]', repl, data)
    else:
        return data
# --- V27.0 æ–°å¢ä¿®æ”¹ç»“æŸ ---


# --- 8. APIè·¯ç”± ---
@app.route('/analyze', methods=['POST', 'OPTIONS'])
@limiter.limit("5 per day")
def analyze_company_text():
    if request.method == 'OPTIONS': return jsonify({'status': 'ok'}), 200
    print("--- v27.0 Clickable Citation Version Analysis request received! ---")
    try:
        data = request.get_json();
        if not data: return jsonify({"error": "Invalid JSON"}), 400

        smart_paste_content = data.get('companyName')
        if not smart_paste_content: return jsonify({"error": "Company name required"}), 400
        
        company_name, job_title, location = extract_entities_with_ai(smart_paste_content)
        if not company_name: return jsonify({"error": "Could not identify company name"}), 400

        context_blocks, source_map, source_id_counter = [], {}, 1
        location_query_part = f' "{location}"' if location else ""
        
        comprehensive_queries = [
            f'"{company_name}"{location_query_part} company culture review', f'"{company_name}"{location_query_part} work life balance',
            f'"{company_name}"{location_query_part} salary benefits', f'"{company_name}"{location_query_part} growth opportunities',
            f'"{company_name}"{location_query_part} hiring process interview', f'"{company_name}"{location_query_part} management style',
            f'"{company_name}"{location_query_part} overtime culture', f'"{company_name}"{location_query_part} innovation culture',
            f'"{company_name}"{location_query_part} diversity inclusion', f'"{company_name}"{location_query_part} training programs',
            f'"{company_name}"{location_query_part} sustainability', f'"{company_name}"{location_query_part} scam fraud',
            f'site:linkedin.com "{company_name}" "{location}"', f'site:indeed.com "{company_name}" "{location}" reviews', f'site:glassdoor.com "{company_name}" "{location}" reviews'
        ]
        
        for query in list(set(comprehensive_queries)):
            snippets, sources_data = perform_google_search(f'{query} after:{datetime.date.today().year - 1}', SEARCH_API_KEY, SEARCH_ENGINE_ID)
            for i, snippet in enumerate(snippets):
                if i < len(sources_data):
                    source_info = sources_data[i]
                    link = source_info.get('link', '').lower()
                    source_info['source_type'] = 'linkedin' if 'linkedin.com' in link else 'glassdoor' if 'glassdoor.com' in link else 'indeed' if 'indeed.com' in link else 'default'
                    context_blocks.append(f"[Source ID: {source_id_counter}] {snippet}")
                    source_map[source_id_counter] = source_info
                    source_id_counter += 1
            time.sleep(0.2) 

        if not context_blocks: return jsonify({"error": "no_info_found"}), 404

        lang_code = data.get('language', 'en')
        language_instructions = {'en': 'English', 'zh-CN': 'Simplified Chinese (ç®€ä½“ä¸­æ–‡)', 'zh-TW': 'Traditional Chinese (ç¹é«”ä¸­æ–‡)'}
        
        full_prompt = PROMPT_TEMPLATE.format(
            output_language=language_instructions.get(lang_code, 'English'), company_name=company_name, 
            job_title=job_title, location=location or "Not Specified",
            current_date=datetime.date.today().strftime("%Y-%m-%d"), resume_text=data.get('resumeText', 'No resume provided.'),
            context_with_sources="\n\n".join(context_blocks)
        )
        
        model = genai.GenerativeModel('gemini-1.5-pro')
        safety_settings = { category: "BLOCK_NONE" for category in ["HARM_CATEGORY_HARASSMENT", "HARM_CATEGORY_HATE_SPEECH", "HARM_CATEGORY_SEXUALLY_EXPLICIT", "HARM_CATEGORY_DANGEROUS_CONTENT"]}
        response = model.generate_content(full_prompt, generation_config=genai.GenerationConfig(response_mime_type="application/json"), safety_settings=safety_settings)
        
        if not response.parts:
            print(f"!!! ä¸»æŠ¥å‘Šç”Ÿæˆè¢«é˜»æ­¢: {response.prompt_feedback} !!!"); return jsonify({"error": "AI response blocked"}), 500

        try:
            ai_json_response = json.loads(response.text)
            report_data = ai_json_response.get("report", {})
            all_mentioned_ids = extract_all_mentioned_ids(report_data)
            valid_ids_set = all_mentioned_ids.intersection(source_map.keys())
            print(f"âœ… éªŒè¯å®Œæˆ: AIæåŠ {len(all_mentioned_ids)}ä¸ªå¼•ç”¨, å…¶ä¸­ {len(valid_ids_set)}ä¸ªæ˜¯æœ‰æ•ˆçš„: {sorted(list(valid_ids_set))}")
            
            # æ­¥éª¤1: æ¸…ç†æ— æ•ˆå¼•ç”¨
            scrubbed_report_data = scrub_invalid_citations(report_data, valid_ids_set)
            print("âœ… æŠ¥å‘Šæ¸…ç†å®Œæˆ: å·²ç§»é™¤æ‰€æœ‰å¹»è§‰å‡ºçš„å¼•ç”¨è§’æ ‡ã€‚")

            # --- V27.0 æ–°å¢ä¿®æ”¹å¼€å§‹ ---
            # æ­¥éª¤2: å°†æœ‰æ•ˆçš„å¼•ç”¨è§’æ ‡è½¬æ¢ä¸ºå¸¦é“¾æ¥çš„Markdownæ ¼å¼
            enriched_report_data = enrich_citations_with_links(scrubbed_report_data, source_map)
            print("ğŸ”— æŠ¥å‘Šé“¾æ¥å¢å¼ºå®Œæˆ: å·²å°†å¼•ç”¨è§’æ ‡è½¬æ¢ä¸ºMarkdowné“¾æ¥ã€‚")
            # --- V27.0 æ–°å¢ä¿®æ”¹ç»“æŸ ---

        except json.JSONDecodeError:
            print(f"!!! Gemini è¿”å›äº†æ— æ•ˆçš„ JSON: {response.text[:500]}... !!!"); return jsonify({"error": "AI failed to generate valid report."}), 500

        final_sources = [ {**source_map[sid], 'id': sid} for sid in sorted(list(valid_ids_set)) if sid in source_map ]
        
        # --- V27.0 ä¿®æ”¹ï¼šè¿”å›å¢å¼ºåçš„æŠ¥å‘Šæ•°æ® ---
        return jsonify({"company_name": company_name, "report": enriched_report_data, "sources": final_sources})

    except Exception as e:
        print(f"!!! å‘ç”ŸæœªçŸ¥é”™è¯¯: {e} !!!"); print(traceback.format_exc()); return jsonify({"error": "Internal server error."}), 500

# --- 9. é”™è¯¯å¤„ç† (æ— å˜åŒ–) ---
@app.errorhandler(429)
def ratelimit_handler(e):
    response = jsonify(error="rate_limit_exceeded")
    response.status_code = 429
    response.headers.add("Access-Control-Allow-Origin", "*")
    return response

# --- 10. å¯åŠ¨ (æ— å˜åŒ–) ---
if __name__ == '__main__':
    app.run(host='0.0.0.0', port=int(os.environ.get("PORT", 8080)), debug=True)
